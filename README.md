# üé¨ MovieLens Recommendation System

This project is capstone 2 assignment for [Machine Learning Zoomcamp 2025](https://datatalks.club/blog/machine-learning-zoomcamp.html)

## üß© Problem
Modern content platforms must help users discover relevant items from large catalogs. 

Without personalization, users face information overload, leading to poor engagement and satisfaction. 

This project addresses the problem of **personalized movie recommendations** based on user‚Äìitem interaction data.

## üéØ Objective
The objective is to build an **end-to-end recommendation system** that:
- Learns user preferences from historical ratings
- Produces personalized recommendations
- Exposes predictions via a REST API
- Is reproducible, containerized, and deployable to cloud infrastructure (GCP)

## üìä Dataset
The project is based on the **MovieLens** dataset, which includes:
- Users
- Movies
- Explicit ratings (user ‚Üí movie ‚Üí rating)

Raw data is processed into intermediate Parquet files for efficient analytics and modeling.

## üìö Theoretical background

### Useful links

* [Build a Recommendation Engine With Collaborative Filtering](https://realpython.com/build-recommendation-engine-collaborative-filtering/)

### What are recommender systems?

Recommender systems are a class of machine learning systems designed to **predict user preferences** and suggest relevant items from a large catalog. They are widely used in domains such as movies, music, e-commerce, news, and advertising.

Formally, the task can be described as estimating a function:

$$
\huge f(u, i) \rightarrow r
$$

where:
- $\large u$ is a user  
- $\large i$ is an item  
- $\large r$ is a relevance score (rating, probability of interaction, or ranking score)

The system then recommends the top-K items with the highest predicted relevance for each user.

---

### Types of recommender systems

#### 1. Content-based filtering

Content-based systems recommend items **similar to those the user liked in the past**, based on item attributes.

**How it works:**
- Build a user profile from item features (genres, tags, descriptions)
- Recommend items with similar features

**Pros:**
- No need for other users
- Works well for cold-start users (with some history)

**Cons:**
- Limited diversity
- Requires rich item metadata
- Cannot discover new tastes easily

---

#### 2. Collaborative filtering

Collaborative filtering relies on **user‚Äìitem interaction patterns** rather than explicit item features.

The core assumption:

> Users who behaved similarly in the past will behave similarly in the future.

There are two main approaches:

---

##### a) Memory-based collaborative filtering

**User-based CF**
- Finds similar users
- Recommends items liked by neighbors

**Item-based CF**
- Finds similar items
- Recommends items similar to those the user interacted with

**Pros:**
- Simple and interpretable
- Effective for small to medium datasets

**Cons:**
- Poor scalability
- Sensitive to sparsity
- Expensive similarity computations

---

##### b) Model-based collaborative filtering

Model-based methods learn **latent representations** of users and items.

The most common approach is **Matrix Factorization**.

---

### Matrix Factorization

User‚Äìitem interactions are represented as a matrix:

$$
\huge R ‚àà ‚Ñù^{|U| √ó |I|}
$$

Matrix factorization decomposes it into two low-rank matrices:

$$
\huge R \approx U \cdot V·µÄ
$$

where:
- $\large U$ represents user latent factors
- $\large V$ represents item latent factors

The dot product $u_i$ ¬∑ $v_j$ gives a predicted relevance score.

---

### Alternating Least Squares (ALS)

ALS is a popular matrix factorization algorithm, especially for **large-scale recommendation systems**.

**Key idea:**
- Fix item factors $\rightarrow$ optimize user factors
- Fix user factors $\rightarrow$ optimize item factors
- Alternate until convergence

**Why ALS?**
- Efficient for sparse matrices
- Easy to parallelize
- Stable and scalable

**Variants:**
- Explicit feedback ALS (ratings)
- Implicit feedback ALS (views, clicks, purchases)

This project uses ALS as the **primary production model**.

---

### Item-based kNN

Item-based kNN is a memory-based collaborative filtering method.

**How it works:**
- Compute item‚Äìitem similarity (cosine, dot product)
- Recommend items similar to those the user has interacted with

**Pros:**
- Simple baseline
- Easy to debug
- Fast inference

**Cons:**
- Does not generalize beyond observed interactions
- Limited performance compared to latent factor models

In this project, item-kNN is used as a **baseline model** for comparison.

---

### Implicit vs Explicit feedback

**Explicit feedback**
- Ratings (1‚Äì5 stars)
- Likes / dislikes

**Implicit feedback**
- Views
- Clicks
- Purchases
- Watch time

Implicit feedback is more common in production systems but noisier. ALS supports both modes.

---

### Key challenges in recommender systems

- **Data sparsity**: most users interact with very few items  
- **Cold start**: new users and new items  
- **Scalability**: millions of users and items  
- **Popularity bias**: over-recommending popular items  
- **Evaluation mismatch**: offline metrics ‚â† online user satisfaction  

This project focuses on **scalable collaborative filtering**, suitable for production deployment, while keeping the architecture extensible for future improvements (hybrid models, online learning, exploration).

The system relies on classical recommender system techniques:
- **Collaborative Filtering**
- **Matrix Factorization**
- **Alternating Least Squares (ALS)** for implicit/explicit feedback
- **Item-based kNN** as a baseline

These approaches assume that users with similar historical behavior will have similar preferences in the future.

## üß† Project's diagram


```mermaid
flowchart TB
    %% Sources
    subgraph Data["Initial Data (Sources)"]
        CSV["CSV files (MovieLens):<br/>users.dat / movies.dat / ratings.dat"]
        PG["PostgreSQL (Cloud SQL / local):<br/>tables users / movies / ratings"]
    end

    %% Prepare
    subgraph Prep["Data preparation"]
        LOAD["Load data (repo / readers)"]
        EDA["EDA:<br/>Exploratory Data Analysis"]
        FEAT["Prepare interactions:<br/>apply rating threshold<br/>encode user_id/movie_id<br/>build X_ui (sparse)"]
        SPLIT["Optional split:<br/>train / val / test (offline eval)"]
    end

    %% Train models
    subgraph Train["Train models"]
        subgraph CF["Collaborative Filtering"]
            KNN["ItemKNN (item-item similarity)"]
            ALS["AlternatingLeastSquares (implicit)"]
        end
        subgraph DL["Deep Learning"]
            PT["PyTorch model (baseline/experiment)"]
        end
    end

    %% Save artifacts
    subgraph Artifacts["Artifacts"]
        KNN_ART["ItemKNN artifacts:<br/>X_ui.npz / S_ii.npz / meta.json"]
        ALS_ART["ALS artifacts:<br/>model.npz / X_ui.npz / mappings.json"]
        PT_ART["PyTorch artifacts:<br/>checkpoint.pt / config.json"]
        MIG["DB migrations (Alembic):<br/>schema for users/movies/ratings"]
    end

    %% Inference
    subgraph Inference["Inference"]
        CLI["CLI script"]
        API["FastAPI web service:<br/>async preload + recommend endpoint"]
    end

    %% Flows: sources -> prep
    CSV --> LOAD
    PG  --> LOAD

    LOAD --> EDA --> FEAT --> SPLIT

    %% Prep -> train
    SPLIT --> Train
    

    %% Train -> artifacts
    KNN --> KNN_ART
    ALS --> ALS_ART
    PT  --> PT_ART

    %% DB-specific artifact
    PG --> MIG

    %% Artifacts -> inference
    Artifacts --> Inference
    
```

## üîç EDA
Exploratory Data Analysis includes:
- Rating distributions
- User and item activity sparsity
- Long-tail effects
- Popularity bias
- Cold-start considerations

All EDA steps are documented in Jupyter notebooks.

## üß™ Model selection
Implemented and compared models:
- ALS (primary production model)
- Item-based kNN (baseline)

Model choice is driven by:
- Scalability
- Recommendation quality
- Training/inference trade-offs

## üìê Evaluation metrics
Offline evaluation uses:
- Precision@K
- Recall@K
- MAP@K
- Coverage (optional)

Metrics are computed on held-out validation splits.

## üíª Tech stack

### Web API

![Python](https://img.shields.io/badge/Python-%3E%3D3.11-3776AB?style=flat-square&logo=python&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-%3E%3D0.128.0-009688?style=flat-square&logo=fastapi&logoColor=white)
![Uvicorn](https://img.shields.io/badge/Uvicorn-%3E%3D0.40.0-111827?style=flat-square&logo=uvicorn&logoColor=white)
![Pydantic](https://img.shields.io/badge/Pydantic-%3E%3D2.12.5-E92063?style=flat-square&logo=pydantic&logoColor=white)
![Pydantic%20Settings](https://img.shields.io/badge/pydantic--settings-%3E%3D2.12.0-E92063?style=flat-square&logo=pydantic&logoColor=white)

![SQLAlchemy](https://img.shields.io/badge/SQLAlchemy-%3E%3D2.0.45-D71F00?style=flat-square&logo=sqlalchemy&logoColor=white)
![Alembic](https://img.shields.io/badge/Alembic-%3E%3D1.18.1-0E7C86?style=flat-square&logo=alembic&logoColor=white)
![psycopg](https://img.shields.io/badge/psycopg-%3E%3D3.3.2-336791?style=flat-square&logo=postgresql&logoColor=white)
![Google%20Cloud%20Storage](https://img.shields.io/badge/Google%20Cloud%20Storage-%3E%3D3.8.0-4285F4?style=flat-square&logo=googlecloud&logoColor=white)
![Click](https://img.shields.io/badge/Click-%3E%3D8.3.1-111827?style=flat-square&logo=python&logoColor=white)

### Machine Learning

![NumPy](https://img.shields.io/badge/NumPy-%3E%3D2.4.0-013243?style=flat-square&logo=numpy&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-%3E%3D2.2.0-150458?style=flat-square&logo=pandas&logoColor=white)
![SciPy](https://img.shields.io/badge/SciPy-%3E%3D1.11.0-8CAAE6?style=flat-square&logo=scipy&logoColor=white)
![scikit--learn](https://img.shields.io/badge/scikit--learn-%3E%3D1.8.0-F7931E?style=flat-square&logo=scikitlearn&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-%3E%3D2.9.1-EE4C2C?style=flat-square&logo=pytorch&logoColor=white)
![Optuna](https://img.shields.io/badge/Optuna-%3E%3D4.6.0-0B5FFF?style=flat-square&logo=optuna&logoColor=white)
![Implicit](https://img.shields.io/badge/implicit-%3E%3D0.7.2-111827?style=flat-square&logo=python&logoColor=white)
![gcsfs](https://img.shields.io/badge/gcsfs-%3E%3D2026.1.0-4285F4?style=flat-square&logo=googlecloud&logoColor=white)

### Infrastructure

![Docker](https://img.shields.io/badge/Docker-28.5.2-2496ED?style=flat-square&logo=docker&logoColor=white)
![Terraform](https://img.shields.io/badge/Terraform-1.7.3-844FBA?style=flat-square&logo=terraform&logoColor=white)
![kubectl](https://img.shields.io/badge/kubectl-1.32.7-326CE5?style=flat-square&logo=kubernetes&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-336791?style=flat-square&logo=postgresql&logoColor=white)
![Google%20Cloud%20Platform](https://img.shields.io/badge/Google%20Cloud%20Platform-GCP-4285F4?style=flat-square&logo=googlecloud&logoColor=white)


## üóÇÔ∏è Project structure

```
‚îú‚îÄ‚îÄ alembic.ini                                 <- Alembic configuration (migrations)
‚îú‚îÄ‚îÄ cloudbuild.yaml                             <- Cloud Build pipeline definition
‚îú‚îÄ‚îÄ data                                        <- Local data directory
‚îÇ   ‚îî‚îÄ‚îÄ interim                                 <- Intermediate / processed datasets for development
‚îÇ       ‚îú‚îÄ‚îÄ movies.parquet.gzip                 <- Processed movie metadata
‚îÇ       ‚îú‚îÄ‚îÄ ratings.parquet.gzip                <- Cleaned user‚Äìitem ratings
‚îÇ       ‚îî‚îÄ‚îÄ users.parquet.gzip                  <- User-level features
‚îú‚îÄ‚îÄ docker-compose.yaml                         <- Local multi-container setup (API/DB/tools)
‚îú‚îÄ‚îÄ Dockerfile                                  <- Image definition for API (and optional ML targets)
‚îú‚îÄ‚îÄ gcp-service-account.json                    <- GCP service account credentials (local use)
‚îú‚îÄ‚îÄ infra                                       <- Infrastructure as code and deployment assets
‚îÇ   ‚îú‚îÄ‚îÄ deploy                                  <- Deployment manifests and Terraform config
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ k8s                                 <- Raw Kubernetes YAML manifests (reference / manual apply)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ configmap.yaml                  <- Runtime configuration for API (env vars)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml                 <- Kubernetes Deployment for API
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ migrate-job.yaml                <- One-off Kubernetes Job for DB migrations (Alembic)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ namespace.yaml                  <- Kubernetes namespace definition
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ secrets.yaml                    <- Kubernetes Secret definitions (DB creds, connection name)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ service.yaml                    <- Kubernetes Service (LoadBalancer) for API
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ serviceaccount.yaml             <- Kubernetes ServiceAccount (Workload Identity binding)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ terraform                           <- Terraform project for GCP + K8s resources
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ apis.tf                         <- GCP APIs enablement
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ artifact_registry.tf            <- Docker Artifact Registry
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ cloudsql.tf                     <- Cloud SQL instance, databases, users
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ gcs.tf                          <- GCS buckets for data and models
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ gke.tf                          <- GKE cluster and node pools
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ iam.tf                          <- IAM roles and bindings (Cloud SQL, GCS, etc.)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ k8s.tf                          <- Kubernetes resources applied via Terraform
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ locals.tf                       <- Shared local variables and manifest maps
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ outputs.tf                      <- Terraform outputs (IPs, names, connection strings)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ providers.tf                    <- Terraform providers configuration
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ secrets.tf                      <- Secrets management (K8s/GCP secrets wiring)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ service_account.tf              <- GCP service accounts for Workload Identity
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ terraform.tfstate               <- Terraform state (local)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ terraform.tfstate.backup        <- Terraform state backup
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ terraform.tfvars                <- Environment-specific variables
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ terraform.tfvars.example        <- Example variables file
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ variables.tf                    <- Input variable definitions
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ versions.tf                     <- Provider and Terraform versions
‚îÇ   ‚îî‚îÄ‚îÄ pgadmin                                 <- Local pgAdmin helper assets
‚îÇ       ‚îú‚îÄ‚îÄ init_pgadmin.sh                     <- pgAdmin initialization script
‚îÇ       ‚îú‚îÄ‚îÄ pgpass                              <- Postgres password file for tooling (local)
‚îÇ       ‚îî‚îÄ‚îÄ servers.json                        <- pgAdmin server connections configuration
‚îú‚îÄ‚îÄ Makefile                                    <- Common automation commands
‚îú‚îÄ‚îÄ mypy.ini                                    <- Static typing configuration
‚îú‚îÄ‚îÄ notebooks                                   <- Research / exploration notebooks
‚îÇ   ‚îú‚îÄ‚îÄ 01_etl.ipynb                            <- Data ingestion & preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ 02_eda.ipynb                            <- Exploratory data analysis
‚îÇ   ‚îú‚îÄ‚îÄ 03_pytorch.ipynb                        <- Experimental modeling (PyTorch)
‚îÇ   ‚îú‚îÄ‚îÄ 03_train.ipynb                          <- Model training workflow
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py                              <- Offline evaluation utilities
‚îú‚îÄ‚îÄ pyproject.toml                              <- Python dependencies and tooling
‚îú‚îÄ‚îÄ README.md                                   <- Project documentation
‚îú‚îÄ‚îÄ recsys                                      <- Application and ML code
‚îÇ   ‚îú‚îÄ‚îÄ aggregates.py                           <- Domain models / aggregates used across the app
‚îÇ   ‚îú‚îÄ‚îÄ alembic                                 <- Alembic migration package
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ env.py                              <- Alembic runtime configuration (online/offline migrations)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README                              <- Notes on migrations setup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ script.py.mako                      <- Migration template
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ versions                            <- Migration revisions
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ 987c10513f96_seed_movielens_ml_1m_data.py <- Seed migration for MovieLens data
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ de48eeb5924c_create_users_movies_ratings.py <- Initial schema migration
‚îÇ   ‚îú‚îÄ‚îÄ api                                     <- FastAPI application layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py                              <- FastAPI app entrypoint (lifespan, routes)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.py                           <- API settings (Pydantic)
‚îÇ   ‚îú‚îÄ‚îÄ config.py                               <- Global configuration helpers/constants
‚îÇ   ‚îú‚îÄ‚îÄ db                                      <- Database layer (SQLAlchemy)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py                           <- ORM models (users/movies/ratings)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repositories                        <- Query/repository layer
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ movies.py                       <- Movies repository (read/query helpers)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ratings.py                      <- Ratings repository (read/query helpers)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ session.py                          <- DB engine/session factory (async)
‚îÇ   ‚îú‚îÄ‚îÄ gcp.py                                  <- Google Cloud Storage / GCP utilities
‚îÇ   ‚îú‚îÄ‚îÄ modeling                                <- Recommender models and training/inference code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ als.py                              <- ALS model wrapper (implicit)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                          <- Dataset / interaction building utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ item_knn.py                         <- Item-based kNN model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict.py                          <- Inference helpers / CLI prediction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ protocols.py                        <- Shared model protocols/interfaces (typing)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ torch.py                            <- PyTorch model baseline/experiments
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train.py                            <- Training entrypoint/workflow
‚îÇ   ‚îú‚îÄ‚îÄ recommender.py                          <- High-level recommender interface (model selection)
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                                <- Shared utilities (I/O, parsing, helpers)
‚îî‚îÄ‚îÄ uv.lock                                     <- Locked Python dependencies
```

## üîå API Contract

### GET /users/{id}/recommendations?k={k}

Top-K recommendations for a user

### Response

```json
{
  "movies": [
    {
      "id": 263,
      "title": "Ladybird Ladybird (1994)",
      "genre": "Drama"
    },
    {
      "id": 612,
      "title": "Pallbearer, The (1996)",
      "genre": "Comedy"
    },
    {
      "id": 1663,
      "title": "Stripes (1981)",
      "genre": "Comedy"
    },
  ]
}
```

## üöÄ  Run

This section explains how to run the project in different environments: locally, via Docker, through the API, notebooks, or the CLI.

In order to run the project, it is necessary to install

* [Docker](https://www.docker.com/)
* [docker-compose](https://docs.docker.com/compose/)
* [uv](https://docs.astral.sh/uv/)
* [Make](https://www.gnu.org/software/make/)

---

### üõ†Ô∏è Makefile

The `Makefile` provides a single entry point for common project tasks such as:

- installing dependencies
- running the API locally
- building Docker images
- running linters and type checks
- triggering training or inference jobs

Using the Makefile helps keep workflows consistent and reproducible.


```makefile
install-kernel:
	uv run python -m ipykernel install --user --name=movielens-recsys --display-name="Recommendation system (MovieLens)"

mypy:
	uv run mypy recsys/

black:
	uv run black --check recsys/

black-fix:
	uv run black recsys/

ruff:
	uv run ruff check recsys/ --fix

lint:
	make mypy & make black-fix & make ruff

api:
	docker-compose up api

jupyter:
	docker-compose up jupyter

train-cli:
	uv run python -m recsys.modeling.train --model-type=$(model_type) --bucket-name=$(bucket_name)

predict-cli:
	uv run python -m recsys.modeling.predict $(user_id) --model-type=$(model_type) --bucket-name=$(bucket_name)
```

---

### üåê API

The recommender system is exposed via a **FastAPI** application.

#### Local

To run the API locally:

* `uv sync`
* Activate virtual environment via `source .venv/bin/activate`
* `python -m recsys.api.app --host 0.0.0.0 --port ${PORT:-8080}`

The API will be available at: 

* http://localhost:8080/docs

---

#### Docker

The API can also be run inside a Docker container: `make api`

This command:
- build the Docker image
- run the container with required environment variables
- expose port `8000`

This setup closely matches the production environment used in Kubernetes.

---

### üìì Jupyter

Jupyter notebooks are used for exploration and experimentation.

#### Local

Run Jupyter locally to work with notebooks from the `notebooks/` directory.  
They cover:

- ETL
- exploratory data analysis
- model training
- evaluation and metrics

In order to run notebooks locally:

* `uv sync`
* Activate virtual environment via `source .venv/bin/activate`
* Create kernel via `make install-kernel`
* Select given kernel in Jupter Notebooks

---

#### Docker

Jupyter can also be launched in Docker for environment isolation and reproducibility, which is useful when sharing experiments or aligning with production dependencies.

It is possible to run Jupyer Notebooks with docker-compose: `make jupyter`

---

### üñ•Ô∏è CLI

CLI entry points are implemented under:

* `make train-cli` - trains particular model
* `make predict-cli` - returns list of recommendations for user with particular model

## ‚òÅÔ∏è Deployment

Service `/docs` could be found [here](http://34.159.178.13/docs)

* It is required to have a [GCP service account](https://docs.cloud.google.com/iam/docs/service-account-overview) in the root of the application's folder
* For making it default auth account, use `gcloud auth application-default login`
  * Make sure, that service account has appropriate roles, like `Cloud Build Service Account`
* After that it is required to build Docker image for GCP structure:

```bash
gcloud builds submit . --config cloudbuild.yaml
```

Then, the deployment itself:

1. Go in `infra/deploy/terraform`
2. `terraform init`
3. (rename `terraform.tfvars.example` => `terraform.tfvars`)
4. `terraform plan -var="apply_k8s=false" -var="apply_migrations=false"` - for creating the infrastructure itself, without applying Kubernetes configs
5. `terraform plan -var="apply_k8s=true" -var="apply_migrations=true"` - Apply Kubernetes configs (with migrations)
6. `terraform output service_external_ip` - You should be able to see external API, which can be used to application's access