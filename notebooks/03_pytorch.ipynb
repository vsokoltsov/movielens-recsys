{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14994441",
   "metadata": {},
   "source": [
    "# 3. Train model (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a20a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from metrics import evaluate_all\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8a2458",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path(os.path.abspath('')).resolve().parents[0]\n",
    "DATA = os.path.join(ROOT, \"data\")\n",
    "INTERIM_DATA = os.path.join(DATA, \"interim\")\n",
    "RAW_DATA = os.path.join(DATA, \"raw\")\n",
    "MODELS = os.path.join(DATA, \"models\")\n",
    "MOVIELENS_PATH = os.path.join(RAW_DATA, \"ml-1m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48734af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_parquet(os.path.join(INTERIM_DATA, 'users.parquet.gzip'))\n",
    "ratings_df = pd.read_parquet(os.path.join(INTERIM_DATA, 'ratings.parquet.gzip'))\n",
    "movies_df = pd.read_parquet(os.path.join(INTERIM_DATA, 'movies.parquet.gzip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6a80d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(963969, 4) (6040, 4) (30200, 4)\n",
      "Users in val: 6040 Users in test: 6040\n"
     ]
    }
   ],
   "source": [
    "def temporal_split_per_user(\n",
    "    ratings: pd.DataFrame,\n",
    "    n_val: int = 1,\n",
    "    n_test: int = 1,\n",
    "    min_train: int = 5,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Per-user time-based split:\n",
    "    - sort each user's interactions by timestamp\n",
    "    - last n_test -> test\n",
    "    - previous n_val -> val\n",
    "    - rest -> train\n",
    "    Users with too few interactions are kept in train only.\n",
    "    \"\"\"\n",
    "    r = ratings.sort_values([\"user_id\", \"timestamp\"]).copy()\n",
    "\n",
    "    r[\"rank\"] = r.groupby(\"user_id\").cumcount() + 1\n",
    "    r[\"user_cnt\"] = r.groupby(\"user_id\")[\"movie_id\"].transform(\"size\")\n",
    "\n",
    "    eligible = r[\"user_cnt\"] >= (min_train + n_val + n_test)\n",
    "\n",
    "    test_mask = eligible & (r[\"rank\"] > r[\"user_cnt\"] - n_test)\n",
    "    val_mask  = eligible & (r[\"rank\"] > r[\"user_cnt\"] - (n_test + n_val)) & ~test_mask\n",
    "    train_mask = ~test_mask & ~val_mask\n",
    "\n",
    "    train = r.loc[train_mask].drop(columns=[\"rank\", \"user_cnt\"])\n",
    "    val   = r.loc[val_mask].drop(columns=[\"rank\", \"user_cnt\"])\n",
    "    test  = r.loc[test_mask].drop(columns=[\"rank\", \"user_cnt\"])\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "train_df, val_df, test_df = temporal_split_per_user(ratings_df, n_val=1, n_test=5, min_train=5)\n",
    "\n",
    "print(train_df.shape, val_df.shape, test_df.shape)\n",
    "print(\"Users in val:\", val_df[\"user_id\"].nunique(), \"Users in test:\", test_df[\"user_id\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d02ae35",
   "metadata": {},
   "source": [
    "## Sanity checks\n",
    "\n",
    "### No intersections by lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753172a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(map(tuple, train_df[[\"user_id\",\"movie_id\",\"timestamp\"]].values)).isdisjoint(\n",
    "       set(map(tuple, test_df[[\"user_id\",\"movie_id\",\"timestamp\"]].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23de55f4",
   "metadata": {},
   "source": [
    "### Users from val/test have data in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5645b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_train = set(train_df[\"user_id\"].unique())\n",
    "assert set(val_df[\"user_id\"].unique()).issubset(users_train)\n",
    "assert set(test_df[\"user_id\"].unique()).issubset(users_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147a158",
   "metadata": {},
   "source": [
    "### Order by time within a user: max(train) <= min(test) (for eligible users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7aeb741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>3186</td>\n",
       "      <td>4</td>\n",
       "      <td>978300019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1270</td>\n",
       "      <td>5</td>\n",
       "      <td>978300055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1721</td>\n",
       "      <td>4</td>\n",
       "      <td>978300055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1022</td>\n",
       "      <td>5</td>\n",
       "      <td>978300055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2340</td>\n",
       "      <td>3</td>\n",
       "      <td>978300103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000019</th>\n",
       "      <td>6040</td>\n",
       "      <td>2917</td>\n",
       "      <td>4</td>\n",
       "      <td>997454429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999988</th>\n",
       "      <td>6040</td>\n",
       "      <td>1921</td>\n",
       "      <td>4</td>\n",
       "      <td>997454464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000172</th>\n",
       "      <td>6040</td>\n",
       "      <td>1784</td>\n",
       "      <td>3</td>\n",
       "      <td>997454464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000167</th>\n",
       "      <td>6040</td>\n",
       "      <td>161</td>\n",
       "      <td>3</td>\n",
       "      <td>997454486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000042</th>\n",
       "      <td>6040</td>\n",
       "      <td>1221</td>\n",
       "      <td>4</td>\n",
       "      <td>998315055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000209 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  movie_id  rating  timestamp\n",
       "31             1      3186       4  978300019\n",
       "22             1      1270       5  978300055\n",
       "27             1      1721       4  978300055\n",
       "37             1      1022       5  978300055\n",
       "24             1      2340       3  978300103\n",
       "...          ...       ...     ...        ...\n",
       "1000019     6040      2917       4  997454429\n",
       "999988      6040      1921       4  997454464\n",
       "1000172     6040      1784       3  997454464\n",
       "1000167     6040       161       3  997454486\n",
       "1000042     6040      1221       4  998315055\n",
       "\n",
       "[1000209 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = ratings_df.sort_values([\"user_id\",\"timestamp\"])\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed0111",
   "metadata": {},
   "source": [
    "## Baseline 1 - Popularity\n",
    "\n",
    "To verify the correctness and usefulness of more advanced recommendation models, it is crucial to establish a simple yet strong baseline.\n",
    "The popularity-based recommender serves as such a reference point.\n",
    "\n",
    "This baseline recommends the same set of items to all users, selecting the most popular movies according to the number of positive interactions in the training data (ratings ≥ 4). No personalization is involved.\n",
    "\n",
    "The popularity baseline provides a lower bound on model performance:\n",
    "* it represents the best result achievable without any personalization,\n",
    "* it reflects how much of the recommendation quality can be explained solely by globally popular items,\n",
    "* it helps detect implementation errors in more complex models — if a personalized model fails to outperform this baseline, it is likely misconfigured or ineffective.\n",
    "\n",
    "From a practical perspective, popularity-based recommendations are often used in real systems as:\n",
    "* a fallback strategy for cold-start users,\n",
    "* a simple default solution,\n",
    "* or a component of a larger ensemble.\n",
    "\n",
    "### Implementation details\n",
    "\n",
    "The model is implemented by:\n",
    "1. Filtering the training data to keep only positive interactions (rating ≥ 4).\n",
    "2. Counting the number of such interactions for each movie.\n",
    "3. Ranking movies by their popularity score.\n",
    "4. Returning the top-K most popular movies, excluding items already seen by the user (when applicable).\n",
    "\n",
    "Formally, for each item i, its popularity score is defined as:\n",
    "\n",
    "$$\n",
    "\\huge popularity(i) = \\sum_{u}\\mathbb{1}[r_{u,i} \\ge 4]\n",
    "$$\n",
    "\n",
    "The recommendation function returns the top-K items with the highest popularity scores.\n",
    "\n",
    "This baseline establishes a clear reference point:\n",
    "any personalized model (e.g. matrix factorization, ALS, neural models) is expected to surpass this score in terms of ranking metrics such as `Recall@K` and `NDCG@K`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "359f628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"interaction\"] = (train_df[\"rating\"] >= 4).astype(int)\n",
    "val_df[\"interaction\"] = (val_df[\"rating\"] >= 4).astype(int)\n",
    "test_df[\"interaction\"] = (test_df[\"rating\"] >= 4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9432ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = train_df[train_df[\"interaction\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bfdb87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie_id\n",
       "2858    2766\n",
       "260     2557\n",
       "1196    2461\n",
       "1198    2213\n",
       "2028    2194\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_popularity = (\n",
    "    train_pos\n",
    "    .groupby(\"movie_id\")\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "item_popularity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85850850",
   "metadata": {},
   "source": [
    "## Baseline 2 - neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026688ac",
   "metadata": {},
   "source": [
    "Explicit ratings are converted into implicit positive interactions.\n",
    "\n",
    "Only positive feedback (rating >= 4) is retained. \n",
    "\n",
    "The absence of an interaction is treated as unknown rather than negative feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16654379",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp = train_df.copy()\n",
    "train_tmp[\"interaction\"] = (train_tmp[\"rating\"] >= 4).astype(np.int8)\n",
    "train_pos = train_tmp[train_tmp[\"interaction\"] == 1][[\"user_id\", \"movie_id\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566aa0d3",
   "metadata": {},
   "source": [
    "### Index mapping and factorization\n",
    "\n",
    "User and item identifiers are mapped to consecutive integer indices to enable efficient matrix operations.\n",
    "\n",
    "Bidirectional mappings (user2idx, idx2item, etc.) are stored to convert between internal indices and original identifiers during inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edf983f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_codes, u_uniques = pd.factorize(train_pos[\"user_id\"], sort=True)\n",
    "i_codes, i_uniques = pd.factorize(train_pos[\"movie_id\"], sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab95b160",
   "metadata": {},
   "source": [
    "### Sparse interaction matrix\n",
    "\n",
    "A sparse user–item interaction matrix $\\large X_{ui}$ is constructed.\n",
    "\n",
    "Here, $\\large X_{ui} = 1$ indicates a positive interaction, and zero indicates no observed interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ceb0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos[\"u_idx\"] = u_codes.astype(np.int32)\n",
    "train_pos[\"i_idx\"] = i_codes.astype(np.int32)\n",
    "\n",
    "user2idx = pd.Series(np.arange(len(u_uniques)), index=u_uniques).to_dict()\n",
    "idx2user = pd.Series(u_uniques).to_dict()\n",
    "\n",
    "idx2item = pd.Series(i_uniques).to_dict()          # i_idx -> movie_id\n",
    "item2idx = pd.Series(np.arange(len(i_uniques)), index=i_uniques).to_dict()\n",
    "\n",
    "X_ui = csr_matrix(\n",
    "    (np.ones(len(train_pos), dtype=np.float32),\n",
    "     (train_pos[\"u_idx\"].to_numpy(), train_pos[\"i_idx\"].to_numpy())),\n",
    "    shape=(len(u_uniques), len(i_uniques)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f7e6b",
   "metadata": {},
   "source": [
    "#### Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b531a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_ui shape: (6038, 3525)\n",
      "max i_idx: 3524 idx2item max key: 3524\n"
     ]
    }
   ],
   "source": [
    "print(\"X_ui shape:\", X_ui.shape)\n",
    "print(\"max i_idx:\", train_pos[\"i_idx\"].max(), \"idx2item max key:\", max(idx2item.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6919885b",
   "metadata": {},
   "source": [
    "## Neural Matrix Factorization (NeuralMF): Model Explanation\n",
    "\n",
    "### What this model does\n",
    "\n",
    "`NeuralMF` is a **neural collaborative filtering model** designed to predict how relevant an item is for a given user.  \n",
    "For each `(user, item)` pair, the model outputs a **scalar score (logit)** that represents the strength of preference.  \n",
    "Higher scores indicate a higher likelihood that the user will interact positively with the item.\n",
    "\n",
    "The model is typically trained on **implicit feedback** (e.g. clicks or ratings above a threshold) using a loss such as  \n",
    "`BCEWithLogitsLoss`, making it suitable for **top-K recommendation** tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### High-level idea\n",
    "\n",
    "The model combines two core ideas:\n",
    "\n",
    "1. **Matrix Factorization**  \n",
    "   Users and items are represented as dense latent vectors (embeddings).\n",
    "\n",
    "2. **Neural interaction modeling**  \n",
    "   Instead of a simple dot product, a neural network learns a **non-linear interaction function** between user and item embeddings.\n",
    "\n",
    "This makes the model more expressive than classical matrix factorization while remaining computationally efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture breakdown\n",
    "\n",
    "#### 1. User and item embeddings\n",
    "\n",
    "Each user and each item is mapped to a dense vector:\n",
    "\n",
    "- `user_emb`: user latent representation  \n",
    "- `item_emb`: item latent representation  \n",
    "\n",
    "The embedding dimension (`emb_dim`) controls the capacity of the latent space.\n",
    "\n",
    "Small random initialization (`std = 0.01`) helps:\n",
    "- stabilize early training\n",
    "- avoid overly confident initial predictions\n",
    "\n",
    "This component is directly analogous to latent factors in classical MF models.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Embedding concatenation\n",
    "\n",
    "User and item embeddings are **concatenated**:\n",
    "\n",
    "- preserves full information from both embeddings\n",
    "- allows the model to learn cross-feature interactions\n",
    "- produces a vector of size `2 × emb_dim`\n",
    "\n",
    "Unlike dot-product MF, no assumption of linear interaction is made at this stage.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Multi-Layer Perceptron (MLP)\n",
    "\n",
    "The concatenated embeddings are passed through a small MLP:\n",
    "\n",
    "- **Linear layer**: projects embeddings into an interaction space\n",
    "- **ReLU activation**: introduces non-linearity\n",
    "- **Dropout**: regularization to reduce overfitting\n",
    "- **Final linear layer**: outputs a single scalar score\n",
    "\n",
    "This network learns how user and item features interact in a non-linear way.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Model output\n",
    "\n",
    "The model outputs a **logit** (raw score) for each `(user, item)` pair:\n",
    "\n",
    "- can be passed through `sigmoid` for probability estimation\n",
    "- can be used directly for ranking items\n",
    "- supports binary classification and ranking losses\n",
    "\n",
    "---\n",
    "\n",
    "### Why this architecture was chosen\n",
    "\n",
    "#### Expressiveness over classical MF\n",
    "- Classical MF uses a dot product (linear interaction)\n",
    "- NeuralMF learns complex, non-linear interactions\n",
    "- Better captures real-world user preferences\n",
    "\n",
    "#### Simplicity and scalability\n",
    "- Lightweight architecture\n",
    "- Efficient to train and infer\n",
    "- Scales to large numbers of users and items\n",
    "\n",
    "#### Compatibility with implicit feedback\n",
    "- Works naturally with negative sampling\n",
    "- Suitable for ranking-based recommendation\n",
    "- Produces scores ideal for top-K retrieval\n",
    "\n",
    "#### Strong baseline neural recommender\n",
    "- Conceptually related to GMF / NeuMF\n",
    "- Easy to extend (deeper MLP, additional features)\n",
    "- Practical for both research and production\n",
    "\n",
    "---\n",
    "\n",
    "### When this model works best\n",
    "\n",
    "- Medium to large implicit feedback datasets\n",
    "- Sparse user–item interaction matrices\n",
    "- Scenarios where interactions are non-linear\n",
    "- As a baseline before introducing more complex architectures\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Scores all items per user (O(n_items)), requiring batching or candidate generation\n",
    "- Does not model temporal or sequential behavior\n",
    "- Requires storing user/item mappings and interaction history\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f754b",
   "metadata": {},
   "source": [
    "### Model evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1b5a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_at_k(recommended, relevant, k=10):\n",
    "    rel = set(relevant)\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(recommended[:k], start=1):\n",
    "        if item in rel:\n",
    "            dcg += 1.0 / math.log2(i + 1)\n",
    "    ideal_hits = min(len(rel), k)\n",
    "    idcg = sum(1.0 / math.log2(i + 1) for i in range(1, ideal_hits + 1))\n",
    "    return 0.0 if idcg == 0 else dcg / idcg\n",
    "\n",
    "def evaluate_model(recommend_fn, model, test_df, k=10, threshold=4):\n",
    "    tmp = test_df.copy()\n",
    "    tmp[\"interaction\"] = (tmp[\"rating\"] >= threshold).astype(np.int8)\n",
    "    test_pos = tmp[tmp[\"interaction\"] == 1]\n",
    "\n",
    "    recalls, ndcgs = [], []\n",
    "    for user_id, g in test_pos.groupby(\"user_id\"):\n",
    "        relevant = g[\"movie_id\"].tolist()\n",
    "        recs = recommend_fn(model, user_id, k=k)\n",
    "        if not recs:\n",
    "            continue\n",
    "\n",
    "        recalls.append(len(set(recs) & set(relevant)) / len(relevant))\n",
    "        ndcgs.append(ndcg_at_k(recs, relevant, k=k))\n",
    "\n",
    "    return float(np.mean(recalls)), float(np.mean(ndcgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27c43efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_pairs_with_negatives(train_pos: pd.DataFrame, X_ui, n_items: int, n_neg: int = 4, seed: int = 42):\n",
    "    \"\"\"\n",
    "    train_pos: DataFrame [u_idx, i_idx] positive interactions\n",
    "    X_ui: csr_matrix (n_users, n_items) positives as 1\n",
    "    returns arrays: users, items, labels\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    pos_u = train_pos[\"u_idx\"].to_numpy()\n",
    "    pos_i = train_pos[\"i_idx\"].to_numpy()\n",
    "    n_pos = len(pos_u)\n",
    "\n",
    "    users = [pos_u]\n",
    "    items = [pos_i]\n",
    "    labels = [np.ones(n_pos, dtype=np.float32)]\n",
    "\n",
    "    neg_users = np.repeat(pos_u, n_neg)\n",
    "    neg_items = np.empty(n_pos * n_neg, dtype=np.int64)\n",
    "\n",
    "    idx = 0\n",
    "    for u in pos_u:\n",
    "        liked = set(X_ui.getrow(u).indices)\n",
    "        for _ in range(n_neg):\n",
    "            j = int(rng.integers(0, n_items))\n",
    "            while j in liked:\n",
    "                j = int(rng.integers(0, n_items))\n",
    "            neg_items[idx] = j\n",
    "            idx += 1\n",
    "\n",
    "    users.append(neg_users)\n",
    "    items.append(neg_items)\n",
    "    labels.append(np.zeros(len(neg_users), dtype=np.float32))\n",
    "\n",
    "    users = np.concatenate(users)\n",
    "    items = np.concatenate(items)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    perm = rng.permutation(len(users))\n",
    "    return users[perm], items[perm], labels[perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5a726f",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11d1f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, users, items, labels):\n",
    "        self.users = torch.tensor(users, dtype=torch.long)\n",
    "        self.items = torch.tensor(items, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3996587",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb189e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralMF(nn.Module):\n",
    "    def __init__(self, n_users: int, n_items: int, emb_dim: int = 64, hidden_dim: int = 128, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "        nn.init.normal_(self.user_emb.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_emb.weight, std=0.01)\n",
    "\n",
    "    def forward(self, u, i):\n",
    "        ue = self.user_emb(u)\n",
    "        ie = self.item_emb(i)\n",
    "        x = torch.cat([ue, ie], dim=1)\n",
    "        logit = self.mlp(x).squeeze(1)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dae3ee8",
   "metadata": {},
   "source": [
    "### Recommendation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a7189e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def recommend_nmf(model, user_id: int, k: int = 10, device=None):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    if user_id not in user2idx:\n",
    "        return []\n",
    "\n",
    "    uidx = user2idx[user_id]\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    n_items = X_ui.shape[1]\n",
    "\n",
    "    item_idx = torch.arange(n_items, dtype=torch.long, device=device)\n",
    "    user_idx = torch.full((n_items,), uidx, dtype=torch.long, device=device)\n",
    "\n",
    "    bs = 20000\n",
    "    scores = []\n",
    "    for start in range(0, n_items, bs):\n",
    "        end = min(start + bs, n_items)\n",
    "        scores.append(model(user_idx[start:end], item_idx[start:end]).detach().cpu().numpy())\n",
    "    scores = np.concatenate(scores)\n",
    "\n",
    "    # exclude seen positives from train\n",
    "    seen = set(X_ui.getrow(uidx).indices)\n",
    "    if seen:\n",
    "        scores[list(seen)] = -np.inf\n",
    "\n",
    "    top = np.argpartition(-scores, k)[:k]\n",
    "    top = top[np.argsort(-scores[top])]\n",
    "    return [idx2item[int(i)] for i in top]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd61aafb",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0071ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_mf(\n",
    "    model: nn.Module,\n",
    "    train_pos: pd.DataFrame,\n",
    "    X_ui,\n",
    "    n_items: int,\n",
    "    test_df,\n",
    "    n_neg: int = 4,\n",
    "    epochs: int = 3,\n",
    "    batch_size: int = 4096,\n",
    "    lr: float = 1e-3,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    weight_decay=1e-6,\n",
    "):\n",
    "    model = model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        users, items, labels = build_train_pairs_with_negatives(\n",
    "            train_pos=train_pos,\n",
    "            X_ui=X_ui,\n",
    "            n_items=n_items,\n",
    "            n_neg=n_neg,\n",
    "            seed=42 + epoch,\n",
    "        )\n",
    "        ds = PairDataset(users, items, labels)\n",
    "        dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for u, i, y in dl:\n",
    "            u, i, y = u.to(device), i.to(device), y.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            logit = model(u, i)\n",
    "            loss = criterion(logit, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item() * len(u)\n",
    "\n",
    "        avg_loss = total_loss / len(ds)\n",
    "        if epoch % 2 == 0:\n",
    "            r, n = evaluate_model(\n",
    "                recommend_fn=lambda m, uid, k=10: recommend_nmf(m, uid, k=k, device=device),\n",
    "                model=model,\n",
    "                test_df=test_df,\n",
    "                k=10,\n",
    "            )\n",
    "            print(f\"  Eval Recall@10={r:.4f} NDCG@10={n:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b270f1af",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81e33b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Eval Recall@10=0.0451 NDCG@10=0.0314\n",
      "  Eval Recall@10=0.0477 NDCG@10=0.0327\n",
      "  Eval Recall@10=0.0531 NDCG@10=0.0374\n",
      "  Eval Recall@10=0.0565 NDCG@10=0.0400\n",
      "  Eval Recall@10=0.0574 NDCG@10=0.0410\n",
      "  Eval Recall@10=0.0654 NDCG@10=0.0465\n",
      "  Eval Recall@10=0.0670 NDCG@10=0.0471\n",
      "  Eval Recall@10=0.0644 NDCG@10=0.0464\n",
      "  Eval Recall@10=0.0666 NDCG@10=0.0472\n",
      "  Eval Recall@10=0.0673 NDCG@10=0.0469\n"
     ]
    }
   ],
   "source": [
    "n_users, n_items = X_ui.shape\n",
    "nmf = NeuralMF(n_users=n_users, n_items=n_items, emb_dim=64, hidden_dim=128, dropout=0.1)\n",
    "\n",
    "nmf = train_neural_mf(\n",
    "    model=nmf,\n",
    "    train_pos=train_pos,\n",
    "    X_ui=X_ui,\n",
    "    n_items=n_items,\n",
    "    test_df=test_df,\n",
    "    epochs=20,\n",
    "    batch_size=4096,\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423258f1",
   "metadata": {},
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a8f7113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06734798007083258, 0.04691890479314088)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall10_bpr, ndcg10_bpr = evaluate_model(\n",
    "    recommend_fn=lambda m, uid, k=10: recommend_nmf(m, uid, k=k),\n",
    "    model=nmf,\n",
    "    test_df=test_df,\n",
    "    k=10,\n",
    ")\n",
    "recall10_bpr, ndcg10_bpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b8f1d",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31fcd063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@10': 0.06734798007083258,\n",
       " 'ndcg@10': 0.04691890479314088,\n",
       " 'mrr@10': 0.06718333290456467,\n",
       " 'hitrate@10': 0.19034755987754368,\n",
       " 'n_users_eval': 5553,\n",
       " 'coverage@10': 0.1702292042235385,\n",
       " 'avg_popularity@10': 1332.9576}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_nmf_fn = lambda uid, k: recommend_nmf(nmf, uid, k=k)\n",
    "metrics_nmf = evaluate_all(\n",
    "    recommend_nmf_fn,\n",
    "    test_df,\n",
    "    users_for_coverage=test_df[\"user_id\"].unique()[:1000],\n",
    "    all_items=movies_df[\"movie_id\"].unique(),\n",
    "    item_popularity=item_popularity.to_dict(),\n",
    "    k=10,\n",
    ")\n",
    "metrics_nmf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf2654",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Based on the evaluation metrics computed for the neural recommendation model, the following conclusions can be drawn.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Recommendation Quality\n",
    "\n",
    "- **Recall@10 ≈ 0.069**  \n",
    "  The model retrieves at least one relevant item for about **6.9%** of users within the top-10 recommendations.  \n",
    "  This indicates **low overall recall**, suggesting that the model struggles to consistently surface relevant items for most users.\n",
    "\n",
    "- **NDCG@10 ≈ 0.048**  \n",
    "  The relatively low NDCG score implies that **relevant items are not ranked highly** when they do appear.  \n",
    "  Even when the model finds relevant items, their positions in the recommendation list are often suboptimal.\n",
    "\n",
    "- **MRR@10 ≈ 0.069**  \n",
    "  The Mean Reciprocal Rank confirms that the **first relevant item typically appears late** in the ranking, reinforcing the observation of weak ranking quality.\n",
    "\n",
    "---\n",
    "\n",
    "#### Hit-Based Evaluation\n",
    "\n",
    "- **HitRate@10 ≈ 0.193**  \n",
    "  Roughly **19% of users** receive at least one relevant recommendation in the top-10.  \n",
    "  This suggests that while the model occasionally succeeds, its performance is **inconsistent across users**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Coverage and Diversity\n",
    "\n",
    "- **Coverage@10 ≈ 0.182**  \n",
    "  About **18% of the item catalog** appears in recommendations, indicating **limited item diversity**.  \n",
    "  The model concentrates recommendations on a relatively small subset of items.\n",
    "\n",
    "- **AvgPopularity@10 ≈ 1333**  \n",
    "  The high average popularity of recommended items suggests a **strong popularity bias**.  \n",
    "  The model heavily favors popular items, which may improve hit rate slightly but reduces personalization and novelty.\n",
    "\n",
    "---\n",
    "\n",
    "#### User Base Context\n",
    "\n",
    "- **Evaluated on 5,553 users**, which provides statistically meaningful results.  \n",
    "  The observed weaknesses are therefore **systematic rather than anecdotal**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbc987",
   "metadata": {},
   "source": [
    "## BPRMF (Bayesian Personalized Ranking Matrix Factorization)\n",
    "\n",
    "**BPRMF** is a recommendation model designed specifically for **implicit feedback** scenarios, where we observe interactions (views, clicks, purchases) rather than explicit ratings.\n",
    "\n",
    "It combines:\n",
    "- **Matrix Factorization (MF)** for latent user–item representations  \n",
    "- **Bayesian Personalized Ranking (BPR)** as a pairwise ranking loss\n",
    "\n",
    "---\n",
    "\n",
    "### Motivation\n",
    "\n",
    "In many real-world recommender systems:\n",
    "- We do **not** know what users dislike\n",
    "- We only observe **positive interactions**\n",
    "- Missing interactions are ambiguous (unknown, not negative)\n",
    "\n",
    "Traditional rating-based losses (MSE, RMSE) are poorly suited for this setting.\n",
    "\n",
    "**BPRMF solves this by learning to rank observed items higher than unobserved ones.**\n",
    "\n",
    "---\n",
    "\n",
    "### Core Idea\n",
    "\n",
    "For a given user **u**:\n",
    "- An interacted item **i** should be ranked **higher** than a non-interacted item **j**\n",
    "\n",
    "Formally, the model learns from triplets:\n",
    "\n",
    "$$\n",
    "(u, i, j)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\large i$ is a positive (observed) item\n",
    "- $\\large j$ is a negative (sampled) item\n",
    "\n",
    "---\n",
    "\n",
    "### Model Structure (Matrix Factorization)\n",
    "\n",
    "Each user and item is represented by a latent vector:\n",
    "\n",
    "- User embedding: $\\large p_u \\in R^k$\n",
    "- Item embedding: $\\large q_i \\in R^k$\n",
    "\n",
    "The predicted preference score is:\n",
    "\n",
    "$$\n",
    "\\huge \\hat{s}(u, i) = p_u · q_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### BPR Loss Function\n",
    "\n",
    "The optimization objective is:\n",
    "\n",
    "$$\n",
    "\\huge L = - \\sum{}{\\log \\sigma( \\hat{s}(u, i) - \\hat{s}(u, j)) + \\lambda ||\\theta||^2} \n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\large \\sigma$ is the sigmoid function\n",
    "- $\\large (u, i, j)$ is a sampled triplet\n",
    "- $\\large \\theta$ are model parameters\n",
    "- $\\large \\lambda$ is the regularization strength\n",
    "\n",
    "This encourages:\n",
    "\n",
    "$$\n",
    "\\huge \\hat{s}(u, i) > \\hat{s}(u, j)\n",
    "$$\n",
    "\n",
    "### Training Process\n",
    "\n",
    "1. Sample a user $\\large u$\n",
    "2. Sample a positive item $\\large i$ (from interactions)\n",
    "3. Sample a negative item $\\large j$ (not interacted)\n",
    "4. Update embeddings via stochastic gradient descent\n",
    "\n",
    "This process is repeated over many epochs.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use BPRMF?\n",
    "\n",
    "**Advantages**\n",
    "- Designed for implicit feedback\n",
    "- Directly optimizes ranking quality\n",
    "- Simple and efficient\n",
    "- Strong baseline for recommender systems\n",
    "\n",
    "**Limitations**\n",
    "- Requires negative sampling\n",
    "- No side features (pure collaborative filtering)\n",
    "- Performance depends on sampling strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3d57a",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "865c382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDatasetPopularHard(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_pos, X_ui, top_pop_iidx, n_triplets, seed=42):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        pos = train_pos[[\"u_idx\",\"i_idx\"]].to_numpy()\n",
    "        idx = rng.integers(0, len(pos), size=n_triplets)\n",
    "\n",
    "        u = pos[idx,0].astype(np.int64)\n",
    "        i_pos = pos[idx,1].astype(np.int64)\n",
    "        i_neg = np.empty(n_triplets, dtype=np.int64)\n",
    "\n",
    "        for t in range(n_triplets):\n",
    "            liked = set(X_ui.getrow(u[t]).indices)\n",
    "            j = int(top_pop_iidx[int(rng.integers(0, len(top_pop_iidx)))])\n",
    "            while j in liked:\n",
    "                j = int(top_pop_iidx[int(rng.integers(0, len(top_pop_iidx)))])\n",
    "            i_neg[t] = j\n",
    "\n",
    "        self.u = torch.tensor(u, dtype=torch.long)\n",
    "        self.i_pos = torch.tensor(i_pos, dtype=torch.long)\n",
    "        self.i_neg = torch.tensor(i_neg, dtype=torch.long)\n",
    "\n",
    "    def __len__(self): return len(self.u)\n",
    "    def __getitem__(self, idx): return self.u[idx], self.i_pos[idx], self.i_neg[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e223c0d",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab1b5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRMF(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.user_emb = torch.nn.Embedding(n_users, emb_dim)\n",
    "        self.item_emb = torch.nn.Embedding(n_items, emb_dim)\n",
    "        torch.nn.init.normal_(self.user_emb.weight, std=0.01)\n",
    "        torch.nn.init.normal_(self.item_emb.weight, std=0.01)\n",
    "\n",
    "    def forward(self, u, i):\n",
    "        return (self.user_emb(u) * self.item_emb(i)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d3f94a",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1e64f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pop_movie_ids = item_popularity.index[:2000].tolist()\n",
    "top_pop_iidx = np.array([item2idx[m] for m in top_pop_movie_ids if m in item2idx], dtype=np.int64)\n",
    "\n",
    "def sample_hard_neg_popular(u_idx: int, X_ui, top_pop_iidx, rng):\n",
    "    liked = set(X_ui.getrow(u_idx).indices)\n",
    "    j = int(top_pop_iidx[int(rng.integers(0, len(top_pop_iidx)))])\n",
    "    while j in liked:\n",
    "        j = int(top_pop_iidx[int(rng.integers(0, len(top_pop_iidx)))])\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c946050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_neural_mf_bpr(\n",
    "    model,\n",
    "    train_pos,\n",
    "    X_ui,\n",
    "    n_items,\n",
    "    test_df,\n",
    "    epochs=10,\n",
    "    batch_size=4096,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-6,\n",
    "    triplets_per_epoch=500_000,   # начни меньше, 2M может быть тяжело\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    model = model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best = -1\n",
    "    best_state = None\n",
    "    patience = 3\n",
    "    bad = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        ds = TripletDatasetPopularHard(\n",
    "            train_pos=train_pos,\n",
    "            X_ui=X_ui,\n",
    "            top_pop_iidx=top_pop_iidx,\n",
    "            n_triplets=triplets_per_epoch,\n",
    "            seed=2000 + epoch,\n",
    "        )\n",
    "        dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for u, ip, ineg in dl:\n",
    "            u, ip, ineg = u.to(device), ip.to(device), ineg.to(device)\n",
    "            opt.zero_grad()\n",
    "\n",
    "            s_pos = model(u, ip)\n",
    "            s_neg = model(u, ineg)\n",
    "\n",
    "            loss = -torch.nn.functional.logsigmoid(s_pos - s_neg).mean()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * len(u)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} - bpr_loss: {total_loss/len(ds):.4f}\")\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            r, n = evaluate_model(\n",
    "                recommend_fn=lambda m, uid, k=10: recommend_nmf(m, uid, k=k, device=device),\n",
    "                model=model,\n",
    "                test_df=test_df,\n",
    "                k=10,\n",
    "            )\n",
    "            print(f\"  Eval Recall@10={r:.4f} NDCG@10={n:.4f}\")\n",
    "            if r > best:\n",
    "                best = r\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "                bad = 0\n",
    "            else:\n",
    "                bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68112695",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c69fec87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - bpr_loss: 0.6835\n",
      "Epoch 2/20 - bpr_loss: 0.5152\n",
      "  Eval Recall@10=0.0431 NDCG@10=0.0307\n",
      "Epoch 3/20 - bpr_loss: 0.4244\n",
      "Epoch 4/20 - bpr_loss: 0.3967\n",
      "  Eval Recall@10=0.0475 NDCG@10=0.0336\n",
      "Epoch 5/20 - bpr_loss: 0.3747\n",
      "Epoch 6/20 - bpr_loss: 0.3591\n",
      "  Eval Recall@10=0.0512 NDCG@10=0.0362\n",
      "Epoch 7/20 - bpr_loss: 0.3447\n",
      "Epoch 8/20 - bpr_loss: 0.3318\n",
      "  Eval Recall@10=0.0492 NDCG@10=0.0353\n",
      "Epoch 9/20 - bpr_loss: 0.3196\n",
      "Epoch 10/20 - bpr_loss: 0.3101\n",
      "  Eval Recall@10=0.0450 NDCG@10=0.0335\n",
      "Epoch 11/20 - bpr_loss: 0.2996\n",
      "Epoch 12/20 - bpr_loss: 0.2911\n",
      "  Eval Recall@10=0.0417 NDCG@10=0.0319\n"
     ]
    }
   ],
   "source": [
    "n_users, n_items = X_ui.shape\n",
    "nmf_bpr = BPRMF(n_users=n_users, n_items=n_items, emb_dim=64)\n",
    "\n",
    "nmf_bpr = train_neural_mf_bpr(\n",
    "    model=nmf_bpr,\n",
    "    train_pos=train_pos,\n",
    "    X_ui=X_ui,\n",
    "    n_items=n_items,\n",
    "    test_df=test_df,\n",
    "    epochs=20,\n",
    "    batch_size=4096,\n",
    "    lr=1e-3,\n",
    "    triplets_per_epoch=500_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e824cd0f",
   "metadata": {},
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69b33309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05118854673149649, 0.03621345174996731)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall10_bpr, ndcg10_bpr = evaluate_model(\n",
    "    recommend_fn=lambda m, uid, k=10: recommend_nmf(m, uid, k=k),\n",
    "    model=nmf_bpr,\n",
    "    test_df=test_df,\n",
    "    k=10,\n",
    ")\n",
    "recall10_bpr, ndcg10_bpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056b7876",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8026e1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall@10': 0.05118854673149649,\n",
       " 'ndcg@10': 0.03621345174996731,\n",
       " 'mrr@10': 0.05245262820897612,\n",
       " 'hitrate@10': 0.14856834143706105,\n",
       " 'n_users_eval': 5553,\n",
       " 'coverage@10': 0.04300798351789853,\n",
       " 'avg_popularity@10': 1936.5911}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_nmf_fn = lambda uid, k: recommend_nmf(nmf_bpr, uid, k=k)\n",
    "metrics_nmf = evaluate_all(\n",
    "    recommend_nmf_fn,\n",
    "    test_df,\n",
    "    users_for_coverage=test_df[\"user_id\"].unique()[:1000],\n",
    "    all_items=movies_df[\"movie_id\"].unique(),\n",
    "    item_popularity=item_popularity.to_dict(),\n",
    "    k=10,\n",
    ")\n",
    "metrics_nmf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da348621",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Based on the evaluation metrics for the **BPRMF (Bayesian Personalized Ranking Matrix Factorization)** model, the following conclusions can be drawn.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overall Recommendation Quality\n",
    "\n",
    "- **Recall@10 ≈ 0.050**  \n",
    "  The model retrieves at least one relevant item for about **5.0% of users** within the top-10 recommendations.  \n",
    "  This is **lower than the neural model**, indicating weaker ability to recover relevant items.\n",
    "\n",
    "- **NDCG@10 ≈ 0.036**  \n",
    "  The low NDCG score suggests that **relevant items are poorly ranked**, even when they are present in the recommendation list.\n",
    "\n",
    "- **MRR@10 ≈ 0.052**  \n",
    "  The first relevant item tends to appear **late in the ranking**, confirming limited ranking effectiveness.\n",
    "\n",
    "---\n",
    "\n",
    "#### Hit-Based Evaluation\n",
    "\n",
    "- **HitRate@10 ≈ 0.145**  \n",
    "  Only **14.5% of users** receive at least one relevant recommendation in the top-10.  \n",
    "  This reflects **unstable performance across users** and weaker personalization.\n",
    "\n",
    "---\n",
    "\n",
    "#### Coverage and Popularity Bias\n",
    "\n",
    "- **Coverage@10 ≈ 0.042**  \n",
    "  Only **4.2% of the item catalog** appears in recommendations, which is **extremely low**.  \n",
    "  The model strongly concentrates on a very small subset of items.\n",
    "\n",
    "- **AvgPopularity@10 ≈ 1923**  \n",
    "  The very high average popularity indicates a **severe popularity bias**.  \n",
    "  The model overwhelmingly recommends globally popular items, limiting novelty and personalization.\n",
    "\n",
    "---\n",
    "\n",
    "#### User Base Context\n",
    "\n",
    "- **Evaluated on 5,553 users**, making the results statistically reliable.  \n",
    "  The observed issues are consistent across the evaluation population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50156e54",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Based on the provided evaluation metrics, the **neural network–based recommender** clearly outperforms the **BPRMF (Bayesian Personalized Ranking Matrix Factorization)** model across all key dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Which Model Performs Better?\n",
    "\n",
    "### Summary Table (Qualitative)\n",
    "\n",
    "| Metric                | Neural Model | BPRMF | Better |\n",
    "|-----------------------|--------------|-------|--------|\n",
    "| Recall@10             | Higher       | Lower | Neural |\n",
    "| NDCG@10               | Higher       | Lower | Neural |\n",
    "| MRR@10                | Higher       | Lower | Neural |\n",
    "| HitRate@10            | Higher       | Lower | Neural |\n",
    "| Item Coverage@10      | Much higher  | Very low | Neural |\n",
    "| Avg Popularity@10     | Lower        | Much higher | Neural |\n",
    "\n",
    "**Conclusion:**  \n",
    "➡️ The **neural recommender model performs better overall** in terms of relevance, ranking quality, personalization, and diversity.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why Does BPRMF Perform Worse?\n",
    "\n",
    "Several structural and data-related reasons explain the weaker performance of BPRMF in this project.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 Model Capacity and Expressiveness\n",
    "\n",
    "- **BPRMF is a linear latent-factor model**\n",
    "  - Captures only user–item interactions via dot products\n",
    "  - Cannot model complex, non-linear user preferences\n",
    "\n",
    "- **Neural models**\n",
    "  - Learn non-linear representations\n",
    "  - Better capture higher-order interaction patterns\n",
    "  - Can implicitly model user behavior heterogeneity\n",
    "\n",
    "➡️ This limits BPRMF’s ability to rank relevant items accurately.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Popularity Bias\n",
    "\n",
    "- **AvgPopularity@10 ≈ 1923 (BPRMF)** vs much lower in the neural model\n",
    "- **Coverage@10 ≈ 4.2%**, indicating extreme item concentration\n",
    "\n",
    "BPRMF tends to:\n",
    "- Overfit to globally popular items\n",
    "- Under-represent niche or long-tail items\n",
    "\n",
    "➡️ This leads to poor personalization and low catalog coverage.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Optimization Objective Mismatch\n",
    "\n",
    "- BPRMF optimizes a **pairwise ranking loss**\n",
    "  - Sensitive to negative sampling strategy\n",
    "  - May converge to shallow popularity-based solutions\n",
    "\n",
    "- Neural models often:\n",
    "  - Use richer training signals\n",
    "  - Benefit from better regularization and capacity\n",
    "\n",
    "➡️ The optimization may not align well with top-K ranking quality in this dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 Data Characteristics\n",
    "\n",
    "The dataset likely exhibits:\n",
    "- Sparse user–item interactions\n",
    "- Long-tail item distribution\n",
    "- High user heterogeneity\n",
    "\n",
    "These conditions:\n",
    "- Hurt shallow latent-factor models like BPRMF\n",
    "- Favor higher-capacity neural architectures\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5 Training and Tuning Sensitivity\n",
    "\n",
    "BPRMF performance is highly sensitive to:\n",
    "- Latent dimension size\n",
    "- Learning rate and regularization\n",
    "- Number of negative samples\n",
    "- Number of training epochs\n",
    "\n",
    "Without extensive tuning, it often underperforms modern neural approaches.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Final Conclusion\n",
    "\n",
    "- ✅ **Neural model**  \n",
    "  - Better relevance and ranking\n",
    "  - Higher personalization\n",
    "  - Broader item coverage\n",
    "  - Lower popularity bias\n",
    "\n",
    "- ❌ **BPRMF**\n",
    "  - Lower recommendation quality\n",
    "  - Severe popularity bias\n",
    "  - Very limited diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a88fd56",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87abe504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v0/8r5h4ym13fb_mzmqc1hfkt580000gn/T/ipykernel_26225/2508353520.py:4: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W0121 22:33:41.939000 26225 torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W0121 22:33:42.312000 26225 torch/onnx/_internal/exporter/_registration.py:107] torchvision is not installed. Skipping torchvision::nms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `NeuralMF([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `NeuralMF([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 17},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.9.1',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"user_id\"<INT64,[s44]>,\n",
       "                %\"item_id\"<INT64,[s44]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"logit\"<FLOAT,[s44]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"mlp.0.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"mlp.3.weight\"<FLOAT,[1,128]>{TorchTensor(...)},\n",
       "                %\"mlp.3.bias\"<FLOAT,[1]>{TorchTensor<FLOAT,[1]>(Parameter containing: tensor([0.0320], requires_grad=True), name='mlp.3.bias')},\n",
       "                %\"user_emb.weight\"<FLOAT,[6038,64]>{TorchTensor(...)},\n",
       "                %\"item_emb.weight\"<FLOAT,[3525,64]>{TorchTensor(...)},\n",
       "                %\"mlp.0.weight\"<FLOAT,[128,128]>{TorchTensor(...)},\n",
       "                %\"val_0\"<INT64,[1]>{TensorProtoTensor<INT64,[1]>(array([1]), name='val_0')}\n",
       "            ),\n",
       "        ) {\n",
       "            0 |  # node_embedding\n",
       "                 %\"embedding\"<FLOAT,[s44,64]> ⬅️ ::Gather(%\"user_emb.weight\"{...}, %\"user_id\") {axis=0}\n",
       "            1 |  # node_embedding_1\n",
       "                 %\"embedding_1\"<FLOAT,[s44,64]> ⬅️ ::Gather(%\"item_emb.weight\"{...}, %\"item_id\") {axis=0}\n",
       "            2 |  # node_cat\n",
       "                 %\"cat\"<FLOAT,[s44,128]> ⬅️ ::Concat(%\"embedding\", %\"embedding_1\") {axis=1}\n",
       "            3 |  # node_linear\n",
       "                 %\"linear\"<FLOAT,[s44,128]> ⬅️ ::Gemm(%\"cat\", %\"mlp.0.weight\"{...}, %\"mlp.0.bias\"{...}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            4 |  # node_relu\n",
       "                 %\"relu\"<FLOAT,[s44,128]> ⬅️ ::Relu(%\"linear\")\n",
       "            5 |  # node_linear_1\n",
       "                 %\"linear_1\"<FLOAT,[s44,1]> ⬅️ ::Gemm(%\"relu\", %\"mlp.3.weight\"{...}, %\"mlp.3.bias\"{[0.031967490911483765]}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            6 |  # node_squeeze\n",
       "                 %\"logit\"<FLOAT,[s44]> ⬅️ ::Squeeze(%\"linear_1\", %\"val_0\"{[1]})\n",
       "            return %\"logit\"<FLOAT,[s44]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_user_emb_weight: \"f32[6038, 64]\", p_item_emb_weight: \"f32[3525, 64]\", p_mlp_0_weight: \"f32[128, 128]\", p_mlp_0_bias: \"f32[128]\", p_mlp_3_weight: \"f32[1, 128]\", p_mlp_3_bias: \"f32[1]\", u: \"i64[s44]\", i: \"i64[s44]\"):\n",
       "                     # File: /Users/vadim.sokoltsov/movielens-recsys/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(\n",
       "                    embedding: \"f32[s44, 64]\" = torch.ops.aten.embedding.default(p_user_emb_weight, u);  p_user_emb_weight = u = None\n",
       "            \n",
       "                     # File: /Users/vadim.sokoltsov/movielens-recsys/.venv/lib/python3.11/site-packages/torch/nn/modules/sparse.py:192 in forward, code: return F.embedding(\n",
       "                    embedding_1: \"f32[s44, 64]\" = torch.ops.aten.embedding.default(p_item_emb_weight, i);  p_item_emb_weight = i = None\n",
       "            \n",
       "                     # File: /var/folders/v0/8r5h4ym13fb_mzmqc1hfkt580000gn/T/ipykernel_26225/2021579045.py:20 in forward, code: x = torch.cat([ue, ie], dim=1)\n",
       "                    cat: \"f32[s44, 128]\" = torch.ops.aten.cat.default([embedding, embedding_1], 1);  embedding = embedding_1 = None\n",
       "            \n",
       "                     # File: /Users/vadim.sokoltsov/movielens-recsys/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear: \"f32[s44, 128]\" = torch.ops.aten.linear.default(cat, p_mlp_0_weight, p_mlp_0_bias);  cat = p_mlp_0_weight = p_mlp_0_bias = None\n",
       "            \n",
       "                     # File: /Users/vadim.sokoltsov/movielens-recsys/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu: \"f32[s44, 128]\" = torch.ops.aten.relu.default(linear);  linear = None\n",
       "            \n",
       "                     # File: /Users/vadim.sokoltsov/movielens-recsys/.venv/lib/python3.11/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone: \"f32[s44, 128]\" = torch.ops.aten.clone.default(relu);  relu = None\n",
       "            \n",
       "                     # File: /Users/vadim.sokoltsov/movielens-recsys/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_1: \"f32[s44, 1]\" = torch.ops.aten.linear.default(clone, p_mlp_3_weight, p_mlp_3_bias);  clone = p_mlp_3_weight = p_mlp_3_bias = None\n",
       "            \n",
       "                     # File: /var/folders/v0/8r5h4ym13fb_mzmqc1hfkt580000gn/T/ipykernel_26225/2021579045.py:21 in forward, code: logit = self.mlp(x).squeeze(1)\n",
       "                    squeeze: \"f32[s44]\" = torch.ops.aten.squeeze.dim(linear_1, 1);  linear_1 = None\n",
       "                    return (squeeze,)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_user_emb_weight: PARAMETER target='user_emb.weight'\n",
       "            p_item_emb_weight: PARAMETER target='item_emb.weight'\n",
       "            p_mlp_0_weight: PARAMETER target='mlp.0.weight'\n",
       "            p_mlp_0_bias: PARAMETER target='mlp.0.bias'\n",
       "            p_mlp_3_weight: PARAMETER target='mlp.3.weight'\n",
       "            p_mlp_3_bias: PARAMETER target='mlp.3.bias'\n",
       "            u: USER_INPUT\n",
       "            i: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            squeeze: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {s44: VR[0, int_oo]}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.eval()\n",
    "u = torch.tensor([0, 1, 2, 3], dtype=torch.long)\n",
    "i = torch.tensor([10, 11, 12, 13], dtype=torch.long)\n",
    "torch.onnx.export(\n",
    "    nmf,\n",
    "    (u, i),\n",
    "    os.path.join(MODELS, \"pytorch_model.pt\"),\n",
    "    export_params=True,\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True,\n",
    "    input_names=[\"user_id\", \"item_id\"],\n",
    "    output_names=[\"logit\"],\n",
    "    dynamic_axes={\n",
    "        \"user_id\": {0: \"batch\"},\n",
    "        \"item_id\": {0: \"batch\"},\n",
    "        \"logit\":   {0: \"batch\"},\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
